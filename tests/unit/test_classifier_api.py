# -*- coding: utf-8 -*-
#
# This file is part of INSPIRE.
# Copyright (C) 2014-2018 CERN.
#
# INSPIRE is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# INSPIRE is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with INSPIRE. If not, see <http://www.gnu.org/licenses/>.
#
# In applying this license, CERN does not waive the privileges and immunities
# granted to it by virtue of its status as an Intergovernmental Organization
# or submit itself to any jurisdiction.

from __future__ import absolute_import, division, print_function

from flask import current_app
from inspire_classifier.api import (
    finetune_and_save_language_model,
    predict_coreness,
    preprocess_and_save_data,
    train_and_save_classifier
)
from mock import patch
import numpy as np
import os
import pandas as pd
import pickle
import requests


TRAINING_DATA = {
0: ["ABC-CNN: An Attention Based Convolutional Neural Network for Visual Question Answering We propose a novel attention based deep learning architecture for visual question answering task (VQA). Given an image and an image related natural language question, VQA generates the natural language answer for the question. Generating the correct answers requires the model's attention to focus on the regions corresponding to the question, because different questions inquire about the attributes of different image regions. We introduce an attention based configurable convolutional neural network (ABC-CNN) to learn such question-guided attention. ABC-CNN determines an attention map for an image-question pair by convolving the image feature map with configurable convolutional kernels derived from the question's semantics. We evaluate the ABC-CNN architecture on three benchmark VQA datasets: Toronto COCO-QA, DAQUAR, and VQA dataset. ABC-CNN model achieves significant improvements over state-of-the-art methods on these datasets. The question-guided attention generated by ABC-CNN is also shown to reflect the regions that are highly relevant to the questions.", 0],
1: ["$SL(2, Z)$ invariant rotating $(m,n)$ strings in $AdS_3\times S^3$ with mixed flux We study rigidly rotating and pulsating (m, n) strings in $AdS_3 \times S^3$ with mixed three form flux. The $AdS_3 \times S^3$ background with mixed three form flux is obtained in the near horizon limit of SL(2, Z)-transformed solution, corresponding to the bound state of NS5-branes and fundamental strings. We study the probe (m, n)-string in this background by solving the manifest SL(2, Z)-covariant form of the action. We find the dyonic giant magnon and single spike solutions corresponding to the equations of motion of a probe string in this background and find various relationships among the conserved charges. We further study a class of pulsating (m, n) string in $AdS_3$ with mixed three form flux.", 2],
2: ["Band head spin assignment of superdeformed bands in A ∼60−80 mass region through nuclear softness formula The nuclear softness formula has been applied to obtain the band head spin (I0) of 7 superdeformed rotational bands (58Ni(1), 58Cu, 62Zn, 65Zn, 68Zn, 84Zr and 86Zr(1)) in A ∼60−80 mass region. To obtain the band head spin (I0) of 7 superdeformed rotational bands in A ∼60−80 mass region least square fitting method is used. The parameters are extracted by fitting the intraband transition energies in the nuclear softness formula from where the root mean deviation (RMD) between the calculated and the observed transition energies are obtained. The calculated transition energies are in good agreement with the experimental transition energies whenever exact band head spin (I0) is assigned. The calculated values of dynamic moment of inertia and its variation with rotational frequency for seven superdeformed rotational bands in A ∼60−80 mass region are also studied. Hence, it is suggested that the nuclear softness formula works very well in A ∼60−80 mass region.", 1],
3: ["ALMA observations of cold molecular gas in AGN hosts at z ∼ 1.5 – evidence of AGN feedback? Similarly to the cosmic star formation history, the black hole accretion rate density of the Universe peaked at 1 < z < 3. This cosmic epoch is hence best suited for investigating the effects of radiative feedback from active galactic nucleus (AGN). Observational efforts are under way to quantify the impact of the AGN feedback, if any, on their host galaxies. Here, we present a study of the molecular gas content of AGN hosts at z ∼ 1.5 using CO[2−1] line emission observed with Atacama Large Millimeter/sub-mm Array (ALMA) for a sample of 10 AGNs. We compare this with a sample of galaxies without an AGN matched in redshift, stellar mass and star formation rate. We detect CO in three AGNs with L_CO ∼ 6.3–25.1 × 10^9 L_⊙, which translates to a molecular hydrogen gas mass of 2.5–10 × 10^10 M_⊙ assuming conventional conversion factor of α_CO ∼ 3.6. Our results indicate a >99 per cent probability of lower depletion time-scales and lower molecular gas fractions in AGN hosts with respect to the non-AGN comparison sample. We discuss the implications of these observations on the impact that AGN feedback may have on star formation efficiency of z >1 galaxies.", 1],
4: ["Ring Theoretic Properties of Partial Crossed products and related themes In this paper we work with unital twisted partial actions. We investigate ring theoretic properties of partial crossed products as artinianity, noetherianity, perfect property, semilocalproperty, semiprimary property and we also study the Krull dimension. Moreover, we consider triangular matrix representation of partial skew group rings, weak and global dimensions of partial crossed products Also we study when the partial crossed products are Frobenius and symmetric algebras.", 0],
5: ["Random walks over a super-percolating two dimensional lattice Two-dimensional networks of ordered quantum dots beyond the percolation threshold are studied, as typical example of conducting nanostructures with quenched random disorder. Theory predicts anomalous diffusion with stretched-exponential relaxation at short distances, and computer simulations on lattices of crossing, straight paths of random length confirm such a behavior. Anomalous diffusion is interpreted as resulting from the higher probability of taking straight, or ballistic paths, when the traveled distance is comparable or shorter than the lattice characteristic length. Diffusion turns over to normal for longer traveled distances, whence all paths tend to become equiprobable. Such random lattice structures represent a model for realistic quantum dot networks, with potential applications in optoelectronics, photovoltaics or spintronics.", 0],
6: ["Anisotropic Bianchi-III cosmological model in $f(R, T)$ gravity An anisotropic Bianchi type-III universe is investigated in the presence of a perfect fluid within the framework of $ f(R,T)$ gravity, where R is the Ricci scalar and T is the trace of the source of matter. Here we have considered the first two cases of the $ f(R,T)$ model, i.e. $ f(R,T)=R+2f(T)$ and $ f(R,T)=f_{1}(R)+f_{2}(T)$ . We have shown that the field equations of $ f(R,T)$ gravity are solvable for any arbitrary function of a scale factor. To get a physically realistic model of the universe, we have assumed a simple power-law form of a scale factor. The exact solutions of the field equations are obtained, which represent an expanding model of the universe which starts expanding with a big bang at $ t = 0$ . The physical behaviours of the model are discussed.", 2],
7: ["Personalized Optimization for Computer Experiments with Environmental Inputs Optimization problems with both control variables and environmental variables arise in many fields. This paper introduces a framework of personalized optimization to han- dle such problems. Unlike traditional robust optimization, personalized optimization devotes to finding a series of optimal control variables for different values of environmental variables. Therefore, the solution from personalized optimization consists of optimal surfaces defined on the domain of the environmental variables. When the environmental variables can be observed or measured, personalized optimization yields more reasonable and better solution- s than robust optimization. The implementation of personalized optimization for complex computer models is discussed. Based on statistical modeling of computer experiments, we provide two algorithms to sequentially design input values for approximating the optimal surfaces. Numerical examples show the effectiveness of our algorithms.", 0],
8: ["Effective Multi-Higgs Couplings to Gluons Standard-Model Higgs bosons are dominantly produced via the gluon-fusion mechanism gg → H at the LHC, i.e. in a loop-mediated process with top loops providing the dominant contribution. For the measured Higgs boson mass of ∼ 125 GeV the limit of heavy top quarks provides a reliable approximation as long as the relative QCD corrections are scaled with the full mass-dependent LO cross section. In this limit the Higgs coupling to gluons can be described by an effective Lagrangian. The same approach can also be applied to the coupling of more than one Higgs boson to gluons. We will derive the effective Lagrangian for multi-Higgs couplings to gluons up to N$^{4}$LO thus extending previous results for more than one Higgs boson. Moreover we discuss gluonic Higgs couplings up to NNLO, if several heavy quarks contribute.", 2],
9: ["Integrable Floquet dynamics We discuss several classes of integrable Floquet systems, i.e. systems whichdo not exhibit chaotic behavior even under a time dependent perturbation. Thefirst class is associated with finite-dimensional Lie groups andinfinite-dimensional generalization thereof. The second class is related to therow transfer matrices of the 2D statistical mechanics models. The third classof models, called here ""boost models"", is constructed as a periodic interchangeof two Hamiltonians - one is the integrable lattice model Hamiltonian, whilethe second is the boost operator. The latter for known cases coincides with theentanglement Hamiltonian and is closely related to the corner transfer matrixof the corresponding 2D statistical models. We present several explicitexamples. As an interesting application of the boost models we discuss apossibility of generating periodically oscillating states with the perioddifferent from that of the driving field. In particular, one can realize anoscillating state by performing a static quench to a boost operator. We termthis state a ""Quantum Boost Clock"". All analyzed setups can be readily realizedexperimentally, for example in cod atoms.", 2],
10: ["First Steps Toward Incorporating Image Based Diagnostics into Particle Accelerator Control Systems Using Convolutional Neural Networks At present, a variety of image-based diagnostics are used in particle accelerator systems. Often times, these are viewed by a human operator who then makes appropriate adjustments to the machine. Given recent advances in using convolutional neural networks (CNNs) for image processing, it should be possible to use image diagnostics directly in control routines (NN-based or otherwise). This is especially appealing for non-intercepting diagnostics that could run continuously during beam operation. Here, we show results of a first step toward implementing such a controller: our trained CNN can predict multiple simulated downstream beam parameters at the Fermilab Accelerator Science and Technology (FAST) facility's low energy beamline using simulated virtual cathode laser images, gun phases, and solenoid strengths.", 1],
11: ["On quantum-mechanical equations of motion in representation dependent of external sources In the present paper, we consider in detail the aspects of the Heisenberg’s equations of motion, related to their transformation to the representation dependent of external sources. We provide with a closed solution as to the variation-derivative motion equations in the general case of a normal form (symbol) chosen. We show that the action in the path integral does depend actually on a particular choice of a normal symbol. We have determined both the aspects of the latter dependence: the specific boundary conditions for virtual trajectories, and the specific boundary terms in the action.", 2],
12: ["Extreme blazars as counterparts of IceCube astrophysical neutrinos We explore the correlation of γ-ray emitting blazars with IceCube neutrinos by using three very recently completed, and independently built, catalogues and the latest neutrino lists. We introduce a new observable, namely the number of neutrino events with at least one γ-ray counterpart, N_ν. In all three catalogues we consistently observe a positive fluctuation of N_ν with respect to the mean random expectation at a significance level of 0.4–1.3 per cent. This applies only to extreme blazars, namely strong, very high energy γ-ray sources of the high energy peaked type, and implies a model-independent fraction of the current IceCube signal ∼10–20 per cent. An investigation of the hybrid photon – neutrino spectral energy distributions of the most likely candidates reveals a set of ≈5 such sources, which could be linked to the corresponding IceCube neutrinos. Other types of blazars, when testable, give null correlation results. Although we could not perform a similar correlation study for Galactic sources, we have also identified two (further) strong Galactic γ-ray sources as most probable counterparts of IceCube neutrinos through their hybrid spectral energy distributions. We have reasons to believe that our blazar results are not constrained by the γ-ray samples but by the neutrino statistics, which means that the detection of more astrophysical neutrinos could turn this first hint into a discovery.", 2],
13: ["Towards resolution of anisotropic cosmological singularity in infinite derivative gravity In this paper, we will show that the equations of motion of the quadratic in curvature, ghost free, infinite derivative theory of gravity will not permit an anisotropic collapse of a homogeneous Universe for a Kasner-type vacuum solution.", 2],
14: ["Sutured Floer homology, fibrations, and taut depth one foliations For an oriented irreducible 3-manifold M with non-empty toroidal boundary, we describe how sutured Floer homology ($SFH$) can be used to determine all fibered classes in $H^1(M)$. Furthermore, we show that the $SFH$ of a balanced sutured manifold $(M,\gamma)$ detects which classes in $H^1(M)$ admit a taut depth one foliation such that the only compact leaves are the components of $R(\gamma)$. The latter had been proved earlier by the first author under the extra assumption that $H_2(M)=0$. The main technical result is that we can obtain an extremal $\text{Spin}^c$-structure $\mathfrak{s}$ (i.e., one that is in a `corner' of the support of $SFH$) via a nice and taut sutured manifold decomposition even when $H_2(M) \neq 0$, assuming the corresponding group $SFH(M,\gamma,\mathfrak{s})$ has non-trivial Euler characteristic.", 0],
15: ["Emergent universe model with dissipative effects Emergent universe model is presented in general theory of relativity with isotropic fluid in addition to viscosity. We obtain cosmological solutions that permit emergent universe scenario in the presence of bulk viscosity that are described by either Eckart theory or Truncated Israel Stewart (TIS) theory. The stability of the solutions are also studied. In this case, the emergent universe (EU) model is analyzed with observational data. In the presence of viscosity, one obtains emergent universe scenario, which however is not permitted in the absence of viscosity. The EU model is compatible with cosmological observations.", 2],
16: ["Mining Software Quality from Software Reviews: Research Trends and Open Issues Software review text fragments have considerably valuable information about users experience. It includes a huge set of properties including the software quality. Opinion mining or sentiment analysis is concerned with analyzing textual user judgments. The application of sentiment analysis on software reviews can find a quantitative value that represents software quality. Although many software quality methods are proposed they are considered difficult to customize and many of them are limited. This article investigates the application of opinion mining as an approach to extract software quality properties. We found that the major issues of software reviews mining using sentiment analysis are due to software lifecycle and the diverse users and teams.", 0],
17: ["Rotational properties of the odd-Z transfermium nucleus$^{255}$Lr by a particle-number-conserving method in the cranked shell model Experimentally observed ground state band based on the 1/2$^{−}$[521] Nilsson state and the first exited band based on the 7/2$^{−}$[514] Nilsson state of the odd-Z nucleus$^{255}$Lr are studied by the cranked shell model (CSM) with the paring correlations treated by the particle-number-conserving (PNC) method. This is the first time the detailed theoretical investigations are performed on these rotational bands. Both experimental kinematic and dynamic moments of inertia (J$^{(1)}$ and J$^{(2)}$) versus rotational frequency are reproduced quite well by the PNC-CSM calculations. By comparing the theoretical kinematic moment of inertia J$^{(1)}$ with the experimental ones extracted from different spin assignments, the spin 17/2$^{−}$ → 13/2$^{−}$ is assigned to the lowest-lying 196.6(5) keV transition of the 1/2$^{−}$[521] band, and 15/2$^{−}$ → 11/2$^{−}$ to the 189(1) keV transition of the 7/2$^{−}$[514] band, respectively. The proton N = 7 major shell is included in the calculations. The intruder of the high-j low-Ω 1j$_{15/2}$ (1/2$^{−}$[770]) orbital at the high spin leads to band-crossings at ħω ≈ 0.20 (ħω ≈ 0.25) MeV for the 7/2$^{−}$[514] α = −1/2 (α = +1/2) band, and at ħω ≈ 0.175 MeV for the 1/2$^{−}$[521] α = −1/2 band, respectively. Further investigations show that the band-crossing frequencies are quadrupole deformation dependent.", 1],
18: ["On the iterative solution of the gap equation in the Nambu-Jona-Lasinio model In this work we revise the standard iterative procedure to find the solution of the gap equation in the Nambu-Jona-Lasinio model within the most popular regularization schemes available in literature in the super-strong coupling regime. We observe that whereas for the hard cut-off regularization schemes, the procedure smoothly converges to the physically relevant solution, Pauli-Villars and Proper-Time regularization schemes become chaotic in the sense of discrete dynamical systems. We call for the need of an appropriate interpretation of the non-convergence of this procedure to the solution of the gap equation.", 2],
19: ["Some Remarks on Glaisher-Ramanujan Type Integrals Some integrals of the Glaisher-Ramanujan type are established in a more general form than in previous studies. As an application we prove some Ramanujan-type series identities, as well as a new formula for the Dirichlet beta function at the value $s=3.$", 0]}

TEST_TITLE = 'Pre-images of extreme points of the numerical range, and applications'
TEST_ABSTRACT = 'We extend the pre-image representation of exposed points of the numerical range of a matrix to all extreme points. With that we characterize extreme points which are multiply generated, having at least two linearly independent pre-images, as the extreme points which are Hausdorff limits of flat boundary portions on numerical ranges of a sequence converging to the given matrix. These studies address the inverse numerical range map and the maximum-entropy inference map which are continuous functions on the numerical range except possibly at certain multiply generated extreme points. This work also allows us to describe closures of subsets of 3-by-3 matrices having the same shape of the numerical range.'



def make_test_dataset():
    test_dataframe = pd.DataFrame.from_dict(TRAINING_DATA, orient='index', columns=['texts', 'coreness'])
    test_dataframe.to_pickle(os.path.join(current_app.config['CLASSIFIER_DATAFRAME_PATH']))


def download_pretrained_wikitext_language_model_and_itos():
    wikitext103_language_model_request = requests.get(current_app.config['CLASSIFIER_WIKITEXT103_LANGUAGE_MODEL_URL'],
                                                   allow_redirects = True)
    open(current_app.config['CLASSIFIER_PRETRAINED_LANGUAGE_MODEL_PATH'], 'wb').write(
        wikitext103_language_model_request.content)

    wikitext103_itos_request = requests.get(current_app.config['CLASSIFIER_WIKITEXT103_ITOS_URL'], allow_redirects=True)
    open(current_app.config['CLASSIFIER_WIKITEXT103_ITOS_PATH'], 'wb').write(wikitext103_itos_request.content)

def test_train():
    config = {
        'CLASSIFIER_DATAFRAME_PATH': '',
        'CLASSIFIER_LANGUAGE_MODEL_DATA_DIR': '',
        'CLASSIFIER_CLASSIFIER_DATA_DIR': '',
        'CLASSIFIER_VALIDATION_DATA_FRACTION': '',
        'CLASSIFIER_CLASSIFICATION_CLASSES': '',
        'CLASSIFIER_DATA_ITOS_PATH': '',
        'CLASSIFIER_MAXIMUM_VOCABULARY_SIZE': '',
        'CLASSIFIER_MINIMUM_WORD_FREQUENCY': '',
        'CLASSIFIER_WIKITEXT103_LANGUAGE_MODEL_URL': 'http://files.fast.ai/models/wt103/fwd_wt103.h5',
        'CLASSIFIER_WIKITEXT103_ITOS_URL': 'http://files.fast.ai/models/wt103/itos_wt103.pkl',
        'CLASSIFIER_PRETRAINED_LANGUAGE_MODEL_PATH': '',
        'CLASSIFIER_WIKITEXT103_ITOS_PATH': '',
        'CLASSIFIER_FINETUNED_LANGUAGE_MODEL_ENCODER_PATH': '',
        'CLASSIFIER_TRAINED_CLASSIFIER_PATH': '',
        'CLASSIFIER_LANGUAGE_MODEL_CYCLE_LENGTH' : 1,
        'CLASSIFIER_CLASSIFIER_CYCLE_LENGTH' : 1
    }

    # Test api:preprocess_and_save_data
    with patch.dict(current_app.config, config):
        make_test_dataset()
        assert os.path.exists(current_app.config['CLASSIFIER_DATAFRAME_PATH'])
        preprocess_and_save_data()

    # Test core/preprocessor:split_and_save_data_for_language_model_and_classifier
    assert os.path.exists(os.path.join(current_app.config['CLASSIFIER_CLASSIFIER_DATA_DIR'], 'train.csv'))
    assert os.path.exists(os.path.join(current_app.config['CLASSIFIER_CLASSIFIER_DATA_DIR'], 'val.csv'))

    classifier_training_csv = pd.read_csv(os.path.join(current_app.config['CLASSIFIER_CLASSIFIER_DATA_DIR'], 'train.csv'))
    assert len(classifier_training_csv) == 18
    classifier_validation_csv = pd.read_csv(os.path.join(current_app.config['CLASSIFIER_CLASSIFIER_DATA_DIR'], 'val.csv'))
    assert len(classifier_validation_csv) == 2

    assert os.path.exists(os.path.join(current_app.config['CLASSIFIER_LANGUAGE_MODEL_DATA_DIR'], 'train.csv'))
    assert os.path.exists(os.path.join(current_app.config['CLASSIFIER_LANGUAGE_MODEL_DATA_DIR'], 'val.csv'))

    language_model_training_csv = pd.read_csv(os.path.join(
        current_app.config['CLASSIFIER_LANGUAGE_MODEL_DATA_DIR'], 'train.csv'))
    assert len(language_model_training_csv) == 18
    language_model_validation_csv = pd.read_csv(os.path.join(
        current_app.config['CLASSIFIER_LANGUAGE_MODEL_DATA_DIR'], 'val.csv'))
    assert len(language_model_validation_csv) == 2

    # Test core/preprocessor:generate_and_save_language_model_tokens
    assert os.path.exists(os.path.join(current_app.config['CLASSIFIER_LANGUAGE_MODEL_DATA_DIR'], 'tok_trn.npy'))
    assert os.path.exists(os.path.join(current_app.config['CLASSIFIER_LANGUAGE_MODEL_DATA_DIR'], 'tok_val.npy'))
    assert os.path.exists(os.path.join(current_app.config['CLASSIFIER_LANGUAGE_MODEL_DATA_DIR'], 'lbl_trn.npy'))
    assert os.path.exists(os.path.join(current_app.config['CLASSIFIER_LANGUAGE_MODEL_DATA_DIR'], 'lbl_val.npy'))

    language_model_training_tokens = np.load(os.path.join(
        current_app.config['CLASSIFIER_LANGUAGE_MODEL_DATA_DIR'], 'tok_trn.npy'))
    assert len(language_model_training_tokens) == 18
    language_model_validation_tokens = np.load(os.path.join(
        current_app.config['CLASSIFIER_LANGUAGE_MODEL_DATA_DIR'], 'tok_val.npy'))
    assert(language_model_validation_tokens) == 2
    language_model_training_labels = np.load(os.path.join(
        current_app.config['CLASSIFIER_LANGUAGE_MODEL_DATA_DIR'], 'lbl_trn.npy'))
    assert len(language_model_training_labels) == 18
    language_model_validation_labels = np.load(os.path.join(
        current_app.config['CLASSIFIER_LANGUAGE_MODEL_DATA_DIR'], 'lbl_val.npy'))
    assert len(language_model_validation_labels) == 2

    # Test core/preprocessor:map_and_save_tokens_to_ids_for_language_model
    assert os.path.exists(current_app.config['CLASSIFIER_DATA_ITOS_PATH'])

    data_itos = pickle.load(open(current_app.config['CLASSIFIER_DATA_ITOS_PATH'], 'rb'))
    assert len(data_itos) == len(current_app.config['CLASSIFIER_MAXIMUM_VOCABULARY_SIZE'])

    assert os.path.exists(os.path.join(current_app.config['CLASSIFIER_LANGUAGE_MODEL_DATA_DIR'], 'trn_ids.npy'))
    assert os.path.exists(os.path.join(current_app.config['CLASSIFIER_LANGUAGE_MODEL_DATA_DIR'], 'val_ids.npy'))

    language_model_training_ids = np.load(os.path.join(
        current_app.config['CLASSIFIER_LANGUAGE_MODEL_DATA_DIR'], 'trn_ids.npy'))
    assert len(language_model_training_ids) == 18
    language_model_validation_ids = np.load(os.path.join(
        current_app.config['CLASSIFIER_LANGUAGE_MODEL_DATA_DIR'], 'val_ids.npy'))
    assert len(language_model_validation_ids) == 2

    # Test core/preprocessor:generate_and_save_classifier_tokens
    assert os.path.exists(os.path.join(current_app.config['CLASSIFIER_CLASSIFIER_DATA_DIR'], 'tok_trn.npy'))
    assert os.path.exists(os.path.join(current_app.config['CLASSIFIER_CLASSIFIER_DATA_DIR'], 'tok_val.npy'))
    assert os.path.exists(os.path.join(current_app.config['CLASSIFIER_CLASSIFIER_DATA_DIR'], 'lbl_trn.npy'))
    assert os.path.exists(os.path.join(current_app.config['CLASSIFIER_CLASSIFIER_DATA_DIR'], 'lbl_val.npy'))

    language_model_training_tokens = np.load(os.path.join(
        current_app.config['CLASSIFIER_CLASSIFIER_DATA_DIR'], 'tok_trn.npy'))
    assert len(language_model_training_tokens) == 18
    language_model_validation_tokens = np.load(os.path.join(
        current_app.config['CLASSIFIER_CLASSIFIER_DATA_DIR'], 'tok_val.npy'))
    assert (language_model_validation_tokens) == 2
    language_model_training_labels = np.load(os.path.join(
        current_app.config['CLASSIFIER_CLASSIFIER_DATA_DIR'], 'lbl_trn.npy'))
    assert len(language_model_training_labels) == 18
    language_model_validation_labels = np.load(os.path.join(
        current_app.config['CLASSIFIER_CLASSIFIER_DATA_DIR'], 'lbl_val.npy'))
    assert len(language_model_validation_labels) == 2

    # Test core/preprocessor:map_and_save_tokens_to_ids_for_classifier
    assert os.path.exists(os.path.join(current_app.config['CLASSIFIER_CLASSIFIER_DATA_DIR'], 'trn_ids.npy'))
    assert os.path.exists(os.path.join(current_app.config['CLASSIFIER_CLASSIFIER_DATA_DIR'], 'val_ids.npy'))

    language_model_training_ids = np.load(os.path.join(
        current_app.config['CLASSIFIER_CLASSIFIER_DATA_DIR'], 'trn_ids.npy'))
    assert len(language_model_training_ids) == 18
    language_model_validation_ids = np.load(os.path.join(
        current_app.config['CLASSIFIER_CLASSIFIER_DATA_DIR'], 'val_ids.npy'))
    assert len(language_model_validation_ids) == 2

    # Test api:finetune_and_save_language_model
    with patch.dict(current_app.config, config):
        download_pretrained_wikitext_language_model_and_itos()
        assert os.path.exists(current_app.config['CLASSIFIER_PRETRAINED_LANGUAGE_MODEL_PATH'])
        assert os.path.exists(current_app.config['CLASSIFIER_WIKITEXT103_ITOS_PATH'])
        finetune_and_save_language_model()

    assert os.path.exists(current_app.config['CLASSIFIER_FINETUNED_LANGUAGE_MODEL_ENCODER_PATH'])

    # Test api:train_and_save_classifier
    with patch.dict(current_app.config, config):
        train_and_save_classifier()

    assert os.path.exists(current_app.config['CLASSIFIER_TRAINED_CLASSIFIER_PATH'])


def test_predict_coreness():
    config = {
        'CLASSIFIER_CLASSIFIER_DATA_DIR': '',
        'CLASSIFIER_CLASSIFICATION_CLASSES': '',
        'CLASSIFIER_DATA_ITOS_PATH': '',
        'CLASSIFIER_TRAINED_CLASSIFIER_PATH': ''
    }

    with patch.dict(current_app.config, config):
        assert  os.path.exists(current_app.config['CLASSIFIER_DATA_ITOS_PATH'])
        assert os.path.exists(current_app.config['CLASSIFIER_TRAINED_CLASSIFIER_PATH'])
        output_dict = predict_coreness(TEST_TITLE, TEST_ABSTRACT)

    assert len(output_dict) == 4
    assert 'predicted_class' in output_dict
    assert 'rejected_score' in output_dict
    assert 'noncore_score' in output_dict
    assert 'core_score' in output_dict
    assert output_dict['rejected_score'] + output_dict['noncore_score'] + output_dict['core_score'] == 1.0